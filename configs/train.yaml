defaults:
  - dataset: lichess-uci-202302
  - llm: chessGPT_d12
  - compile: no_options

run:
  project: train_transformer
  data_dir: data
  loader: # parameters for dataloader
    batch_size: 5 # loader batch size
    num_workers: 4 # num workers for loader
  hf_dataset_num_proc: 4
  force_dtype: bfloat16
  lr_find: false
  lr: 0.0001

trainer:
  _target_: lightning.pytorch.trainer.trainer.Trainer
  # accumulate_grad_batches: 10
  fast_dev_run: false
  gradient_clip_val: 0.5
  # limit_predict_batches: ???
  # limit_train_batches: ???
  limit_test_batches: 0.0
  limit_val_batches: 0.0 # number of val batches to use
  max_epochs: 1
  # num_nodes: 1 # number of nodes for distributed training
  # val_check_interval: 10000
  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: train/loss
      mode: min
    - _target_: lightning.pytorch.callbacks.OnExceptionCheckpoint
      dirpath: ${hydra:runtime.output_dir}/exception
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}/checkpoint
      train_time_interval:
        _target_: datetime.timedelta
        minutes: 30.0
      save_top_k: 1
    - _target_: src.callbacks.NtfyCallback
  logger:
    - _target_: lightning.pytorch.loggers.WandbLogger
      name: ${now:%Y-%m-%d}/${now:%H-%M-%S}
      project: ${hydra:job.name}
      log_model: false
      checkpoint_name: null
    - _target_: lightning.pytorch.loggers.CSVLogger
      save_dir: ${hydra:runtime.output_dir}
      name: lightning_logs
      version: null
      prefix: ""
      flush_logs_every_n_steps: 100

hydra:
  job_logging:
    root:
      level: INFO
