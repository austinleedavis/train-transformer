defaults:
  - dataset: lichess-uci-synthetic
  - llm: chessGPT_d12

run:
  project: train_transformer
  data_dir: data
  loader: # parameters for dataloader
    batch_size: 2 # loader batch size
    num_workers: 4 # num workers for loader
  hf_dataset_num_proc: 4

loss_fn:
  _target_: torch.nn.CrossEntropyLoss
lr: 0.0001

compile:
  options: null
  # options:
  #   triton.cudagraphs: True
  #   shape_padding: True

trainer:
  _target_: lightning.pytorch.trainer.trainer.Trainer
  # accumulate_grad_batches: 10
  fast_dev_run: false
  gradient_clip_val: 0.5
  max_epochs: 1
  num_nodes: 1 # number of nodes for distributed training
  callbacks:
    - _target_: lightning.pytorch.callbacks.BatchSizeFinder
      mode: binsearch
      steps_per_trial: 10
      init_val: 2
      batch_arg_name: batch_size
  logger:
    - _target_: lightning.pytorch.loggers.CSVLogger
      save_dir: ${hydra:runtime.output_dir}
      name: lightning_logs
      version: null
      prefix: ""
      flush_logs_every_n_steps: 100

hydra:
  job_logging:
    root:
      level: INFO
